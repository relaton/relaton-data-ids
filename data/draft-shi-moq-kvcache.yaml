---
id: draftshimoqkvcache
schema_version: v1.4.1
formattedref: draft-shi-moq-kvcache
title:
- language: en
  script: Latn
  content: KVCache over MoQT
docidentifier:
- content: draft-shi-moq-kvcache
  type: Internet-Draft
  primary: true
abstract:
- language: en
  script: Latn
  content: "   Large language model (LLM) inference involves two stages: prefill and\n
    \  decode.  The prefill phase processes the prompt in parallel,\n   generating
    the KVCache, which is then used by the decode phase to\n   produce tokens sequentially.
    \ KVCache can be reused if the model and\n   prompt is the same, reducing computing
    cost of the prefill.  However,\n   its large size makes efficient transfer challenging.
    \ Delivering\n   these over architectures enabled by publish/subscribe transport
    like\n   MoQT, allows local nodes to cache the KVCache to be later retrieved\n
    \  via new subscriptions, saving the bandwidth.  This document specifies\n   the
    transmission of KVCache over MoQT.\n\n\t "
relation:
- type: includes
  bibitem:
    formattedref: draft-shi-moq-kvcache-00
    source:
    - type: src
      content: https://datatracker.ietf.org/doc/html/draft-shi-moq-kvcache-00
    docidentifier:
    - content: draft-shi-moq-kvcache-00
      type: Internet-Draft
      primary: true
- type: includes
  bibitem:
    formattedref: draft-shi-moq-kvcache-01
    source:
    - type: src
      content: https://datatracker.ietf.org/doc/html/draft-shi-moq-kvcache-01
    docidentifier:
    - content: draft-shi-moq-kvcache-01
      type: Internet-Draft
      primary: true
