---
id: draftliunmrgaillminferencerequirements00
type: standard
schema_version: v1.4.1
title:
- language: en
  script: Latn
  content: Requirements Analysis of System and Network for Large Language Model Inference
    Service
source:
- type: src
  content: https://datatracker.ietf.org/doc/html/draft-liu-nmrg-ai-llm-inference-requirements-00
docidentifier:
- content: draft-liu-nmrg-ai-llm-inference-requirements
  type: Internet-Draft
- content: draft-liu-nmrg-ai-llm-inference-requirements-00
  type: Internet-Draft
  primary: true
docnumber: I-D.liu-nmrg-ai-llm-inference-requirements
date:
- type: published
  at: '2025-03-03'
contributor:
- role:
  - type: author
  person:
    name:
      forename:
      - language: en
        script: Latn
        content: Liu
      - language: en
        script: Latn
        initial: L
      formatted_initials:
        language: en
        content: L.
      surname:
        language: en
        content: Chang
      completename:
        language: en
        content: Liu Chang
    affiliation:
    - organization:
        name:
        - language: en
          content: China Mobile
- role:
  - type: author
  person:
    name:
      forename:
      - language: en
        script: Latn
        content: Chuyi
      - language: en
        script: Latn
        initial: C
      formatted_initials:
        language: en
        content: C.
      surname:
        language: en
        content: Guo
      completename:
        language: en
        content: Chuyi Guo
    affiliation:
    - organization:
        name:
        - language: en
          content: China Mobile
version:
- draft: '00'
language:
- en
script:
- Latn
abstract:
- language: en
  script: Latn
  content: "   With the rise of ChatGPT, DeepSeek, and other Large Language Models,\n
    \  which is short for LLMs in the remaining part, as well as the\n   proliferation
    of inference applications, inference serving oriented\n   to large-scale users
    has become increasingly critical.  However, due\n   to the extreme demands on
    computing power and communication during\n   inference, the large-scale service
    deployment of LLMs poses\n   significant challenges.  To address these challenges,
    different\n   vendors have adopted diverse inference service architectures, among\n
    \  which the vLLM proposed in 2023 is the most representative.  This\n   paper
    investigates mainstream inference frameworks, summarizes their\n   core design
    principles, and analyzes the requirements and challenges\n   they impose on system
    and network configurations.  The goal is to lay\n   a foundation for defining
    a unified LLM inference architecture in the\n   future.\n\n\n\t "
relation:
- type: updatedBy
  bibitem:
    formattedref: draft-liu-nmrg-ai-llm-inference-requirements-01
    source:
    - type: src
      content: https://datatracker.ietf.org/doc/html/draft-liu-nmrg-ai-llm-inference-requirements-01
    docidentifier:
    - content: draft-liu-nmrg-ai-llm-inference-requirements-01
      type: Internet-Draft
      primary: true
series:
- type: main
  title:
  - language: en
    script: Latn
    content: Internet-Draft
  number: draft-liu-nmrg-ai-llm-inference-requirements-00
ext:
  schema_version: v1.0.1
  doctype:
    content: internet-draft
  flavor: ietf
