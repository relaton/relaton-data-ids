---
id: draftxurtgwgfareinsun01
type: standard
schema_version: v1.4.1
title:
- language: en
  script: Latn
  content: Fully Adaptive Routing Ethernet in Scale-Up Networks
source:
- type: src
  content: https://datatracker.ietf.org/doc/html/draft-xu-rtgwg-fare-in-sun-01
docidentifier:
- content: draft-xu-rtgwg-fare-in-sun
  type: Internet-Draft
- content: draft-xu-rtgwg-fare-in-sun-01
  type: Internet-Draft
  primary: true
docnumber: I-D.xu-rtgwg-fare-in-sun
date:
- type: published
  at: '2025-05-21'
contributor:
- role:
  - type: author
  person:
    name:
      forename:
      - language: en
        script: Latn
        content: Xiaohu
      - language: en
        script: Latn
        initial: X
      formatted_initials:
        language: en
        content: X.
      surname:
        language: en
        content: Xu
      completename:
        language: en
        content: Xiaohu Xu
    affiliation:
    - organization:
        name:
        - language: en
          content: China Mobile
- role:
  - type: author
  person:
    name:
      forename:
      - language: en
        script: Latn
        content: Zongying
      - language: en
        script: Latn
        initial: Z
      formatted_initials:
        language: en
        content: Z.
      surname:
        language: en
        content: He
      completename:
        language: en
        content: Zongying He
    affiliation:
    - organization:
        name:
        - language: en
          content: Broadcom
- role:
  - type: author
  person:
    name:
      forename:
      - language: en
        script: Latn
        content: Hua
      - language: en
        script: Latn
        initial: H
      formatted_initials:
        language: en
        content: H.
      surname:
        language: en
        content: Wang
      completename:
        language: en
        content: Hua Wang
    affiliation:
    - organization:
        name:
        - language: en
          content: Moore Threads
- role:
  - type: author
  person:
    name:
      forename:
      - language: en
        script: Latn
        content: Tianyou
      - language: en
        script: Latn
        initial: T
      formatted_initials:
        language: en
        content: T.
      surname:
        language: en
        content: Zhou
      completename:
        language: en
        content: Tianyou Zhou
    affiliation:
    - organization:
        name:
        - language: en
          content: Resnics Technology
- role:
  - type: author
  person:
    name:
      forename:
      - language: en
        script: Latn
        content: Yongtao
      - language: en
        script: Latn
        initial: "Y"
      formatted_initials:
        language: en
        content: Y.
      surname:
        language: en
        content: Yang
      completename:
        language: en
        content: Yongtao Yang
    affiliation:
    - organization:
        name:
        - language: en
          content: Centec
- role:
  - type: author
  person:
    name:
      forename:
      - language: en
        script: Latn
        content: Yinben
      - language: en
        script: Latn
        initial: "Y"
      formatted_initials:
        language: en
        content: Y.
      surname:
        language: en
        content: Xia
      completename:
        language: en
        content: Yinben Xia
    affiliation:
    - organization:
        name:
        - language: en
          content: Tencent
- role:
  - type: author
  person:
    name:
      forename:
      - language: en
        script: Latn
        content: Peilong
      - language: en
        script: Latn
        initial: P
      formatted_initials:
        language: en
        content: P.
      surname:
        language: en
        content: Wang
      completename:
        language: en
        content: Peilong Wang
    affiliation:
    - organization:
        name:
        - language: en
          content: Baidu
- role:
  - type: author
  person:
    name:
      forename:
      - language: en
        script: Latn
        content: Yan
      - language: en
        script: Latn
        initial: "Y"
      formatted_initials:
        language: en
        content: Y.
      surname:
        language: en
        content: Zhuang
      completename:
        language: en
        content: Yan Zhuang
    affiliation:
    - organization:
        name:
        - language: en
          content: Huawei Technologies
- role:
  - type: author
  person:
    name:
      forename:
      - language: en
        script: Latn
        content: Fajie
      - language: en
        script: Latn
        initial: F
      formatted_initials:
        language: en
        content: F.
      surname:
        language: en
        content: Yang
      completename:
        language: en
        content: Fajie Yang
    affiliation:
    - organization:
        name:
        - language: en
          content: Cloudnine Information Technologies
- role:
  - type: author
  person:
    name:
      forename:
      - language: en
        script: Latn
        content: Chao
      - language: en
        script: Latn
        initial: C
      formatted_initials:
        language: en
        content: C.
      surname:
        language: en
        content: Li
      completename:
        language: en
        content: Chao Li
    affiliation:
    - organization:
        name:
        - language: en
          content: Metanet Networking Technology
- role:
  - type: author
  person:
    name:
      forename:
      - language: en
        script: Latn
        content: Xiaojun
      - language: en
        script: Latn
        initial: X
      formatted_initials:
        language: en
        content: X.
      surname:
        language: en
        content: Wang
      completename:
        language: en
        content: Xiaojun Wang
    affiliation:
    - organization:
        name:
        - language: en
          content: Ruijie Networks
version:
- draft: '01'
language:
- en
script:
- Latn
abstract:
- language: en
  script: Latn
  content: "   The Mixture of Experts (MoE) has become a dominant paradigm in\n   transformer-based
    artificial intelligence (AI) large language models\n   (LLMs).  It is widely adopted
    in both distributed training and\n   distributed inference.  Furthermore, the
    disaggregation of the\n   prefill and decode phases is highly beneficial and is
    considered a\n   best practice for distributed inference models; however, this\n
    \  approach depends on highly efficient Key-Value (KV) cache\n   synchronization.
    \ To enable efficient expert parallelization and KV\n   cache synchronization
    across dozens or even hundreds of Graphics\n   Processing Units (GPUs) in MoE
    architectures, an ultra-high-\n   throughput, ultra-low-latency AI scale-up network
    (SUN) that can\n   efficiently distribute data across all network planes is critical.\n
    \  This document describes how to extend the Weighted Equal-Cost Multi-\n   Path
    (WECMP) load-balancing mechanism, referred to as Fully Adaptive\n   Routing Ethernet
    (FARE), which was originally designed for scale-out\n   networks, to scale-up
    networks.\n\n\n\t "
relation:
- type: updates
  bibitem:
    formattedref: draft-xu-rtgwg-fare-in-sun-00
    source:
    - type: src
      content: https://datatracker.ietf.org/doc/html/draft-xu-rtgwg-fare-in-sun-00
    docidentifier:
    - content: draft-xu-rtgwg-fare-in-sun-00
      type: Internet-Draft
      primary: true
series:
- type: main
  title:
  - language: en
    script: Latn
    content: Internet-Draft
  number: draft-xu-rtgwg-fare-in-sun-01
ext:
  schema_version: v1.0.1
  doctype:
    content: internet-draft
  flavor: ietf
