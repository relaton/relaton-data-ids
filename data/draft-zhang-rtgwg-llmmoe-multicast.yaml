---
id: draftzhangrtgwgllmmoemulticast
schema_version: v1.4.1
formattedref: draft-zhang-rtgwg-llmmoe-multicast
title:
- language: en
  script: Latn
  content: Multicast usage in LLM MoE
docidentifier:
- content: draft-zhang-rtgwg-llmmoe-multicast
  type: Internet-Draft
  primary: true
abstract:
- language: en
  script: Latn
  content: "   Large Language Models (LLMs) have been widely used in recent years.\n
    \  The Mixture of Experts (MoE) architecture is one of the features of\n   LLMs
    that enables efficient inference and cost-effective training.\n   With the MoE
    architecture, there are potential multicast use cases\n   such as tokens dispatching.
    \ This draft attempts to analyze these use\n   cases.\n\n\t "
relation:
- type: includes
  bibitem:
    formattedref: draft-zhang-rtgwg-llmmoe-multicast-00
    source:
    - type: src
      content: https://datatracker.ietf.org/doc/html/draft-zhang-rtgwg-llmmoe-multicast-00
    docidentifier:
    - content: draft-zhang-rtgwg-llmmoe-multicast-00
      type: Internet-Draft
      primary: true
- type: includes
  bibitem:
    formattedref: draft-zhang-rtgwg-llmmoe-multicast-01
    source:
    - type: src
      content: https://datatracker.ietf.org/doc/html/draft-zhang-rtgwg-llmmoe-multicast-01
    docidentifier:
    - content: draft-zhang-rtgwg-llmmoe-multicast-01
      type: Internet-Draft
      primary: true
