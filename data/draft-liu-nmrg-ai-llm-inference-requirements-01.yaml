---
id: draftliunmrgaillminferencerequirements01
type: standard
schema_version: v1.4.1
title:
- language: en
  script: Latn
  content: Requirements Analysis of System and Network for Large Language Model Inference
    Service
source:
- type: src
  content: https://datatracker.ietf.org/doc/html/draft-liu-nmrg-ai-llm-inference-requirements-01
docidentifier:
- content: draft-liu-nmrg-ai-llm-inference-requirements
  type: Internet-Draft
- content: draft-liu-nmrg-ai-llm-inference-requirements-01
  type: Internet-Draft
  primary: true
docnumber: I-D.liu-nmrg-ai-llm-inference-requirements
date:
- type: published
  at: '2025-07-07'
contributor:
- role:
  - type: author
  person:
    name:
      forename:
      - language: en
        script: Latn
        content: Liu
      - language: en
        script: Latn
        initial: L
      formatted_initials:
        language: en
        content: L.
      surname:
        language: en
        content: Chang
      completename:
        language: en
        content: Liu Chang
    affiliation:
    - organization:
        name:
        - language: en
          content: China Mobile
- role:
  - type: author
  person:
    name:
      forename:
      - language: en
        script: Latn
        content: Chuyi
      - language: en
        script: Latn
        initial: C
      formatted_initials:
        language: en
        content: C.
      surname:
        language: en
        content: Guo
      completename:
        language: en
        content: Chuyi Guo
    affiliation:
    - organization:
        name:
        - language: en
          content: China Mobile
version:
- draft: '01'
language:
- en
script:
- Latn
abstract:
- language: en
  script: Latn
  content: "   With the rise of ChatGPT, DeepSeek, and other Large Language Models,\n
    \  which is short for LLMs in the remaining part, as well as the\n   proliferation
    of inference applications, inference serving oriented\n   to large-scale users
    has become increasingly critical.  However, due\n   to the extreme demands on
    computing power and communication during\n   inference, the large-scale service
    deployment of LLMs poses\n   significant challenges.  To address these challenges,
    different\n   vendors have adopted diverse inference service architectures, such
    as\n   vLLM, SGLang, Mooncake, etc.  This paper investigates mainstream\n   inference
    frameworks, summarizes their core design principle and\n   research question,
    and analyzes the challenges and requirements they\n   impose on network management.
    \ The goal is to lay a foundation for\n   defining a unified LLM inference architecture
    in the future.\n\n\n\t "
relation:
- type: updates
  bibitem:
    formattedref: draft-liu-nmrg-ai-llm-inference-requirements-00
    source:
    - type: src
      content: https://datatracker.ietf.org/doc/html/draft-liu-nmrg-ai-llm-inference-requirements-00
    docidentifier:
    - content: draft-liu-nmrg-ai-llm-inference-requirements-00
      type: Internet-Draft
      primary: true
- type: updatedBy
  bibitem:
    formattedref: draft-liu-nmrg-ai-llm-inference-requirements-02
    source:
    - type: src
      content: https://datatracker.ietf.org/doc/html/draft-liu-nmrg-ai-llm-inference-requirements-02
    docidentifier:
    - content: draft-liu-nmrg-ai-llm-inference-requirements-02
      type: Internet-Draft
      primary: true
series:
- type: main
  title:
  - language: en
    script: Latn
    content: Internet-Draft
  number: draft-liu-nmrg-ai-llm-inference-requirements-01
ext:
  schema_version: v1.0.1
  doctype:
    content: internet-draft
  flavor: ietf
