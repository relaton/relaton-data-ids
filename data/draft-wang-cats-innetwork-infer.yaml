---
schema-version: v1.2.9
id: draft-wang-cats-innetwork-infer
title:
- content: In-Network Intelligence for Distributed Collaborative Inference Acceleration
  language:
  - en
  script:
  - Latn
  format: text/plain
docid:
- id: draft-wang-cats-innetwork-infer
  type: Internet-Draft
  primary: true
abstract:
- content: "<p>The rapid proliferation of deep learning models has led to growing
    demands for low-latency and high-throughput inference across heterogeneous environments.
    While edge devices often host data sources, their limited compute and network
    resources restrict efficient model inference. Cloud servers provide abundant capacity
    but suffer from transmission delays and bottlenecks. Emerging programmable in-network
    devices (e.g., switches, FPGAs, SmartNICs) offer a unique opportunity to accelerate
    inference by processing tasks directly along data paths. This document introduces
    an architecture for _Distributed Collaborative Inference Acceleration_. It proposes
    mechanisms to split, offload, and coordinate inference workloads across edge devices,
    in-network resources, and cloud servers, enabling reduced response time and improved
    utilization.</p>"
  language:
  - en
  script:
  - Latn
  format: text/html
relation:
- type: includes
  bibitem:
    id: draft-wang-cats-innetwork-infer-00
    docid:
    - id: draft-wang-cats-innetwork-infer-00
      type: Internet-Draft
      primary: true
    formattedref:
      content: draft-wang-cats-innetwork-infer-00
      format: text/plain
