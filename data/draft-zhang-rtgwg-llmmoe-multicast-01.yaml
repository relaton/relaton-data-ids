---
id: draftzhangrtgwgllmmoemulticast01
type: standard
schema_version: v1.4.1
title:
- language: en
  script: Latn
  content: Multicast usage in LLM MoE
source:
- type: src
  content: https://datatracker.ietf.org/doc/html/draft-zhang-rtgwg-llmmoe-multicast-01
docidentifier:
- content: draft-zhang-rtgwg-llmmoe-multicast
  type: Internet-Draft
- content: draft-zhang-rtgwg-llmmoe-multicast-01
  type: Internet-Draft
  primary: true
docnumber: I-D.zhang-rtgwg-llmmoe-multicast
date:
- type: published
  at: '2025-10-20'
contributor:
- role:
  - type: author
  person:
    name:
      forename:
      - language: en
        script: Latn
        content: Zheng
      - language: en
        script: Latn
        initial: Z
      formatted_initials:
        language: en
        content: Z.
      surname:
        language: en
        content: Zhang
      completename:
        language: en
        content: Zheng Zhang
    affiliation:
    - organization:
        name:
        - language: en
          content: ZTE Corporation
- role:
  - type: author
  person:
    name:
      forename:
      - language: en
        script: Latn
        content: Wei
      - language: en
        script: Latn
        initial: W
      formatted_initials:
        language: en
        content: W.
      surname:
        language: en
        content: Duan
      completename:
        language: en
        content: Wei Duan
    affiliation:
    - organization:
        name:
        - language: en
          content: ZTE Corporation
- role:
  - type: author
  person:
    name:
      forename:
      - language: en
        script: Latn
        content: Xiaohu
      - language: en
        script: Latn
        initial: X
      formatted_initials:
        language: en
        content: X.
      surname:
        language: en
        content: Xu
      completename:
        language: en
        content: Xiaohu Xu
    affiliation:
    - organization:
        name:
        - language: en
          content: China Mobile
version:
- draft: '01'
language:
- en
script:
- Latn
abstract:
- language: en
  script: Latn
  content: "   Large Language Models (LLMs) have been widely used in recent years.\n
    \  The Mixture of Experts (MoE) architecture is one of the features of\n   LLMs
    that enables efficient inference and cost-effective training.\n   With the MoE
    architecture, there are potential multicast use cases\n   such as tokens dispatching.
    \ This draft attempts to analyze these use\n   cases.\n\n\t "
relation:
- type: updates
  bibitem:
    formattedref: draft-zhang-rtgwg-llmmoe-multicast-00
    source:
    - type: src
      content: https://datatracker.ietf.org/doc/html/draft-zhang-rtgwg-llmmoe-multicast-00
    docidentifier:
    - content: draft-zhang-rtgwg-llmmoe-multicast-00
      type: Internet-Draft
      primary: true
series:
- type: main
  title:
  - language: en
    script: Latn
    content: Internet-Draft
  number: draft-zhang-rtgwg-llmmoe-multicast-01
ext:
  schema_version: v1.0.1
  doctype:
    content: internet-draft
  flavor: ietf
